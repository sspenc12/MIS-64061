---
title: "Assignment 2 - RNN on IMDB Dataset"
output:
  word_document: default
  html_notebook:
    highlight: textmate
    theme: cerulean
---

Assignment 2 -- Recurrent Neural Network (RNN) - IMDB

Consider the IMDB example from Chapter 6. Re-run the example modifying the following:

Cutoff reviews after 150 words
Restrict training samples to 100
Validate on 10,000 samples
Consider only the top 10,000 words

Consider both a embedding layer, and a pretrained word embedding.  Which approach did better? Now try changing the number of training samples to determine at what point the embedding layer gives better performance.

Initially, the pre-trained word embedding model (GloVe in this case) was outperformed by the single embedding layer model. However, as the summary chart below shows, the single embedded layer model surpassed the pre-trained model when the training sample size exceeded 750.

See code below that was utilized to gather the performance results for each level of training samples.

IMDB dataset was downloaded from: "http://mng.bz/0tIo"
The GloVe pre-trained model was downloaded from: "https://nlp.stanford.edu/projects/glove"

Download and tokenize the IMDB dataset with this chunk of code. "Training_Sample" was altered from this chunk of code to increase the value and observe the results.

```{r eval=FALSE, include=TRUE}

# Download the IMDB dataset from "http://mng.bz/0tIo"

require(keras)

imdb_dir <- "aclImdb"
train_dir <- file.path(imdb_dir, "train")

labels <- c()
texts <- c()

for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(train_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"),
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
                           }
}

maxlen <- 150 # Cutoff reviews after 150 words
training_samples <- 100 # Train on 100 samples
validation_samples <- 10000 # Validates on 10,000 samples
max_words <- 10000 # Consider only the top 10,000 words in the dataset

tokenizer <- text_tokenizer(num_words = max_words) %>%
  fit_text_tokenizer(texts)

sequences <- texts_to_sequences(tokenizer, texts)

word_index = tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")

data <- pad_sequences(sequences, maxlen = maxlen)

labels <- as.array(labels)
cat("Shape of data tensor:", dim(data), "\n")
cat("Shape of label tensor:", dim(labels), "\n")

indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1): (training_samples + validation_samples)]

x_train <- data[training_indices, ]
y_train <- labels[training_indices]

x_val <- data[validation_indices, ]
y_val <- labels[validation_indices]

```

Download and integrate the GloVe embedding information via this chunk. There were a total of 400,000 word vectors.

```{r eval=FALSE, include=TRUE}

# Download GloVe Embeddings from "https://nlp.stanford.edu/projects/glove"

glove_dir <- "glove"
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))

embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)) {
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}

cat("Found", length(embeddings_index), "word vectors.\n")

```

Pre-process and integrate the word embeddings from GloVe via this chunk of code.

```{r eval=FALSE, include=TRUE}

# Preprocess the embeddings

embedding_dim <- 100

embedding_matrix <- array(0, c(max_words, embedding_dim))

for (word in names(word_index)) {
  index <- word_index[[word]]
  if (index < max_words) {
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector))
      embedding_matrix[index + 1, ] <- embedding_vector
  }
}
```

Chunk of code utilized for the pre-trained model utilizing GloVe and the single-layer embedding model.

```{r eval=FALSE, include=TRUE}

# Defining a pre-trained model

model_pretrained <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, input_length = maxlen) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

summary(model_pretrained)

# Defining a model with embedding layer that has not been pre-trained

model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, input_length = maxlen) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

summary(model)

```

This chunk of code was used to integrate the weights from GloVe and freeze the weights for this pre-trained model.

```{r eval=FALSE, include=TRUE}

# load pre-trained embedding layer

get_layer(model_pretrained, index = 1) %>%
  set_weights(list(embedding_matrix)) %>%
  freeze_weights()

```

Train the model and verify the validation accuracy and loss for the pretrained model.

```{r eval=FALSE, include=TRUE}

set.seed(322020)

# Training and evaluation of pre-trained model

model_pretrained %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model_pretrained %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

```

Train the model and verify the validation accuracy and loss for the single embedding layer model.

```{r eval=FALSE, include=TRUE}


set.seed(322020)

# Training and evaluation of model without pre-training

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

```

Summary of results are shown in the table at the beginning of the file.
